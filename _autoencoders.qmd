## Autoencoders

This section was written by Kyle Reed, a senior at the University of 
Connecticut double majoring in Applied Data Analysis and Geographic 
Information Science.

This section will explore:

- What an Autoencoder is

- How an Autoencoder works

- Potential applications of autoencoders

### Introduction

Autoencoders are a specialized type of neural network used in unsupervised 
learning. They are trained to encode input data into a compressed 
representation and then decode it back to something as close as possible to the 
original. This process forces the model to learn the most important features or 
patterns in the data.

Unlike traditional supervised learning models, autoencoders do not require 
labeled data. Instead, the model learns from the data itself by minimizing the 
difference between the original input and the reconstructed output.

### How it works

An autoencoder consists of two primary components:

- Encoder: Deconstructs the input data into a lower-dimensional 
representation.

- Decoder: Reconstructs the original input from the compressed encoding.

#### Encoder

The encoder compresses data into smaller lower-dimensional groupings. It learns 
the data and identifies the most essential features of the data while 
discarding redundant information.

#### Decoder

The decoder reconstructs the data set from the compressed analyzed data 
generated by the encoder. The decoder attempts to reproduce the data as 
closely as possible by reversing the compression process.


### Application

Autoencoders can be used for:

- Data Compression
  Reduce dataset size for storage and transmission while retaining key 
  information.

- Anomaly Detection 
  Identify unusual patterns that differ from the learned norm based on 
  reconstruction error.

- Image/Audio Refining  
  Remove noise, fill missing pixels or sound samples, colorize images, and more.

- Data Refining/Denoising  
  Improve dataset quality by correcting errors and filling missing values.

### Example usage

For the example, I wish show how autoencoders can be used to compress data and 
refine data/images.
The mnist data set will be used which is a collection of various numbers 
that are drawn out on a small pixel image.

#### Load and Prepare Data

```{python}
#| echo: true
#| results: "hide"

# Import necessary packages
import torch
from torch import nn, optim
from torchvision import datasets, transforms
import numpy as np
import matplotlib.pyplot as plt

# Load data from the MNIST dataset
transform = transforms.Compose([
    transforms.ToTensor(), 
    transforms.Lambda(lambda x: x.view(-1))
])

# Download training and testing data
train_data = datasets.MNIST(
    root='./data', 
    train=True, 
    download=True, 
    transform=transform
)

test_data = datasets.MNIST(
    root='./data', 
    train=False, 
    download=True, 
    transform=transform
)

train_loader = torch.utils.data.DataLoader(
    train_data, 
    batch_size=256, 
    shuffle=True
)

test_loader = torch.utils.data.DataLoader(
    test_data, 
    batch_size=256, 
    shuffle=False
)
```

Here I used `torch` package which is one of the most common and easy 
to use frameworks people use for building and training autoencoders.

Data is broken into train and test data, and converted to floating-point 
format.

Reshaping images as seen in the last two lines allows to neural network to
compress the data easier when it is one-dimensional.

#### Build The Encoder

```{python}
#| echo: true
#| results: "hide"

# Define the autoencoder using PyTorch's nn.Module
class Autoencoder(nn.Module):
    def __init__(self):
        super(Autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(28 * 28, 256),  # Initial encoding layer
            nn.ReLU(),
            nn.Linear(256, 128),      # Bottleneck layer (compressed version)
            nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(128, 256),      # Reconstruction layer
            nn.ReLU(),
            nn.Linear(256, 28 * 28),  # Final reconstructed version
            nn.Sigmoid()              
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded

# Instantiate the autoencoder model
autoencoder = Autoencoder()
```


For each of the activation lines, the number represents the number of neurons 
for each layer, so '256' means that this layer transforms the input into a 
256-dimensional representation

When choosing the number of neurons, you want to pick a number that fits your 
data well. More neurons are needed for larger complex models, but they are not 
necessary for smaller, less complex models as this could cause overfitting.

#### Train the encoder

```{python}
#| echo: true
#| results: "hide"

# Prepare the model, optimizer, and loss function
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
autoencoder = autoencoder.to(device)
criterion = nn.BCELoss()
optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)

# Train the autoencoder
num_epochs = 40
for epoch in range(num_epochs):
    autoencoder.train()
    running_loss = 0.0
    for data in train_loader:
        inputs, _ = data
        inputs = inputs.to(device)

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = autoencoder(inputs)
        loss = criterion(outputs, inputs)

        # Backward pass and optimize
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
```

For the model preparation I used “Adam,” which stands for Adaptive Moment 
Estimation. This is a very popular optimizer for autoencoders that works well 
with noisy or large data sets. 

For training: 'epochs' = number of full passes through the data

#### Reconstruct The Data

```{python}
#| echo: true
#| results: "hide"

# Reconstruct the images using the trained autoencoder
autoencoder.eval()
reconstructed_images = []
with torch.no_grad():
    for data in test_loader:
        inputs, _ = data
        inputs = inputs.to(device)
        outputs = autoencoder(inputs)
        reconstructed_images.append(outputs)

# Convert the reconstructed images to a tensor
reconstructed_images = torch.cat(reconstructed_images, dim=0)

# Plot the original and reconstructed images
n = 10  # Number of images to display
plt.figure(figsize=(16, 4))
for i in range(n):
    # Top plot for original images
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(test_data[i][0].view(28, 28).cpu(), cmap="gray")
    plt.title("Original")
    plt.axis("off")
    
    # Bottom plot for reconstructed images
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(reconstructed_images[i].view(28, 28).cpu(), cmap="gray")
    plt.title("Reconstructed")
    plt.axis("off")
plt.show()
```

The output should show the reconstructed image above the original image.

### Conclusion

- Autoencoders compress and reconstruct data, enabling pattern recognition 
without labeled data.

- Useful in tasks like anomaly detection, data cleaning, and feature extraction.

- Training involves minimizing reconstruction error, using loss functions such 
as MSE.

### Further Readings

- Doersch's [*Tutorial on Variational Autoencoders*](@doersch2016vae)
- Michelucci's [*Deep Learning with TensorFlow 2*](@micelucci2022tensorflow)
